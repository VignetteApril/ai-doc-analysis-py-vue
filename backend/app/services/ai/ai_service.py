import os
import json
import re
import uuid
import logging
from typing import TypedDict, List, Dict, Optional, Tuple
from openai import AsyncOpenAI
from langgraph.graph import StateGraph, END

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("æ³°å±±Agent")


class AgentState(TypedDict):
    html_content: str
    clean_text: str
    raw_issues: list
    final_issues: list
    final_html: str


class AIService:
    def __init__(self):
        self.client = AsyncOpenAI(
            api_key=os.getenv("AI_API_KEY"),
            base_url=os.getenv("AI_BASE_URL")
        )
        self.model = os.getenv("AI_MODEL_NAME", "pro/deepseek-ai/DeepSeek-V3")

        self.SKELETON_CHAR_PATTERN = re.compile(r"[\u4e00-\u9fa5a-zA-Z0-9]")
        self.NON_SKELETON_PATTERN = re.compile(r"[^\u4e00-\u9fa5a-zA-Z0-9]")

        self.graph = self._build_workflow()
        logger.info(f"ğŸš€ AI Service Ready | Model: {self.model}")

    # ====================== èŠ‚ç‚¹é€»è¾‘ ======================

    async def preprocess_node(self, state: AgentState):
        logger.info("ğŸŸ¢ [1. é¢„å¤„ç†] æå–çº¯æ–‡æœ¬...")
        text = re.sub(r'<[^>]+>', '', state.get("html_content", ""))
        return {"clean_text": text}

    async def scanner_node(self, state: AgentState):
        logger.info("ğŸŸ¡ [2. åˆå®¡] æ‰«æé”™è¯¯...")

        prompt = """ä½ æ˜¯ä¸€åèµ„æ·±å…¬æ–‡æ ¡å¯¹å‘˜ã€‚è¯·é˜…è¯»åŸæ–‡ï¼Œæ‰¾å‡ºã€é”™åˆ«å­—ã€æ ‡ç‚¹é”™è¯¯ã€è¯­ç—…ã€æ•æ„Ÿè¯ã€ä¸è§„èŒƒè¡¨è¾¾ã€‘ã€‚

        ### è¾“å‡ºè¦æ±‚ï¼š
        è¿”å›ä¸¥æ ¼çš„ JSON å¯¹è±¡ï¼ˆä¸è¦ä½¿ç”¨ Markdown ä»£ç å—ï¼‰ï¼ŒåŒ…å« `issues` æ•°ç»„ã€‚

        ### æ•°ç»„å…ƒç´ å­—æ®µï¼š
        1. `original`: åŸæ–‡ä¸­é”™è¯¯çš„ç‰‡æ®µï¼ˆå¿…é¡»ä¸åŸæ–‡å®Œå…¨ä¸€è‡´ï¼‰
        2. `content`: ä¿®æ”¹åçš„å»ºè®®å†…å®¹ï¼ˆå¦‚æœæ˜¯åˆ é™¤å»ºè®®ï¼Œè¯·è¿”å›ç©ºå­—ç¬¦ä¸² ""ï¼‰
        3. `type`: é”™è¯¯ç±»å‹ï¼ˆé”™åˆ«å­—ã€æ ‡ç‚¹é”™è¯¯ã€è¯­ç—…ç­‰ï¼‰
        4. `reason`: ä¿®æ”¹ç†ç”±ï¼ˆç®€æ´æ¸…æ™°ï¼‰

        ### è¾“å‡ºç¤ºä¾‹ï¼š
        {
            "issues": [
                {"original": "è®°è¦", "content": "çºªè¦", "type": "é”™åˆ«å­—", "reason": "ç”¨è¯é”™è¯¯"},
                {"original": "ï¼Œï¼Œ", "content": "ï¼Œ", "type": "æ ‡ç‚¹é”™è¯¯", "reason": "é‡å¤æ ‡ç‚¹"}
            ]
        }
        """

        try:
            data = await self._call_ai("Scanner", prompt, state["clean_text"])
            issues = data.get("issues", []) if isinstance(data, dict) else data
            if not isinstance(issues, list):
                issues = []
            logger.info(f"âœ… Scanner å‘ç° {len(issues)} ä¸ªé—®é¢˜")
            return {"raw_issues": issues}
        except Exception as e:
            logger.error(f"âŒ Scanner å¼‚å¸¸: {e}")
            return {"raw_issues": []}

    async def reviewer_node(self, state: AgentState):
        logger.info(f"ğŸŸ  [3. å¤å®¡] å¤„ç†å»ºè®® (å…± {len(state['raw_issues'])} æ¡)...")
        if not state["raw_issues"]:
            return {"final_issues": []}

        approved = []
        for issue in state["raw_issues"]:
            issue_id = f"ai-{uuid.uuid4().hex[:10]}"
            approved.append({
                "id": issue_id,
                "original": issue.get("original", ""),
                "content": issue.get("content") or issue.get("suggestion", ""),
                "type": issue.get("type", "å…¶ä»–"),
                "message": issue.get("reason", issue.get("message", "")),   # å‰ç«¯éœ€è¦ message
                "reason": issue.get("reason", "")   # ä¿ç•™å…¼å®¹
            })
        return {"final_issues": approved}

    def finalizer_node(self, state: AgentState):
        logger.info("ğŸ”µ [4. ç»ˆå®¡] æ³¨å…¥ AI æ ¡é˜…æ ‡è®°...")

        current_html = state["html_content"]
        valid_issues = []

        for issue in state["final_issues"]:
            orig = issue.get("original", "").strip()
            if not orig:
                continue

            loc = self._get_python_indices(current_html, orig)
            if loc:
                start, end = loc
                issue_id = issue["id"]

                tag_start = (
                    f'<span class="ai-correction-mark" '
                    f'data-ai-id="{issue_id}" '
                    f'title="AI å»ºè®®ï¼š{issue.get("message", "")}">'
                )
                tag_end = '</span>'

                before = current_html[:start]
                target = current_html[start:end]
                after = current_html[end:]

                current_html = before + tag_start + target + tag_end + after

                valid_issues.append(issue)
                logger.debug(f"ğŸ“ æ³¨å…¥æˆåŠŸ: {orig} â†’ data-ai-id={issue_id}")
            else:
                logger.warning(f"âŒ æ— æ³•å®šä½åŸæ–‡: '{orig}'")

        logger.info(f"ğŸ æ³¨å…¥å®Œæˆï¼Œç”Ÿæ•ˆ {len(valid_issues)} æ¡æ ‡è®°")
        return {
            "final_issues": valid_issues,
            "final_html": current_html
        }

    # ====================== å®šä½ç®—æ³• ======================

    def _get_python_indices(self, full_text: str, target: str) -> Optional[Tuple[int, int]]:
        """å¢å¼ºç‰ˆå®šä½ï¼šä¼˜å…ˆç²¾ç¡®åŒ¹é… â†’ éª¨æ¶åŒ¹é…"""
        if not target or not full_text:
            return None

        idx = full_text.find(target)
        if idx != -1:
            return idx, idx + len(target)

        target_sk = self.NON_SKELETON_PATTERN.sub("", target)
        if len(target_sk) < 2:
            return None

        doc_sk = []
        doc_map = []

        is_in_tag = False
        for i, char in enumerate(full_text):
            if char == '<':
                is_in_tag = True
            elif char == '>':
                is_in_tag = False
            elif not is_in_tag and self.SKELETON_CHAR_PATTERN.match(char):
                doc_sk.append(char)
                doc_map.append(i)

        sk_str = "".join(doc_sk)
        sk_idx = sk_str.find(target_sk)

        if sk_idx != -1:
            real_start = doc_map[sk_idx]
            real_end = doc_map[sk_idx + len(target_sk) - 1] + 1
            return real_start, real_end

        return None

    # ====================== è¾…åŠ©æ–¹æ³• ======================

    async def _call_ai(self, tag, sys_prompt, user_content):
        try:
            resp = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_content}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
                timeout=150
            )
            raw = resp.choices[0].message.content.strip()
            if "```" in raw:
                raw = re.sub(r"^```(?:json)?\s*", "", raw)
                raw = re.sub(r"\s*```$", "", raw)
            return json.loads(raw)
        except Exception as e:
            logger.error(f"[{tag}] AI è°ƒç”¨å¤±è´¥: {e}")
            raise

    def _build_workflow(self):
        wf = StateGraph(AgentState)
        wf.add_node("preprocess", self.preprocess_node)
        wf.add_node("scan", self.scanner_node)
        wf.add_node("review", self.reviewer_node)
        wf.add_node("finalize", self.finalizer_node)

        wf.set_entry_point("preprocess")
        wf.add_edge("preprocess", "scan")
        wf.add_edge("scan", "review")
        wf.add_edge("review", "finalize")
        wf.add_edge("finalize", END)
        return wf.compile()

    async def analyze_stream(self, html_content: str):
        # å…³é”®ä¿®æ”¹ï¼šæ ‡å‡† SSE æ ¼å¼ï¼Œæ¯æ¡æ¶ˆæ¯ä»¥ data: {json}\n\n ç»“æŸ
        yield f"data: {json.dumps({'step': 'start', 'desc': 'AI æ ¡å®¡å¯åŠ¨...'})}\n\n"

        initial_state = {
            "html_content": html_content,
            "clean_text": "",
            "raw_issues": [],
            "final_issues": [],
            "final_html": ""
        }

        final_result = {}
        async for event in self.graph.astream(initial_state):
            for node_name, output in event.items():
                if node_name == "finalize":
                    final_result = output
                desc_map = {
                    "preprocess": "æå–çº¯æ–‡æœ¬",
                    "scan": "æ·±åº¦æ‰«æé—®é¢˜",
                    "review": "å¤æ ¸å»ºè®®",
                    "finalize": "æ³¨å…¥æ ¡é˜…æ ‡è®°"
                }
                yield f"data: {json.dumps({'step': node_name, 'desc': desc_map.get(node_name, node_name)})}\n\n"

        payload = {
            "step": "complete",
            "results": {
                "final_issues": final_result.get("final_issues", []),
                "final_html": final_result.get("final_html", "")
            }
        }
        logger.info(f"ğŸ“¤ æ ¡å®¡å®Œæˆ | å‘ç° {len(payload['results']['final_issues'])} ä¸ªé—®é¢˜")
        yield f"data: {json.dumps(payload)}\n\n"


# å•ä¾‹
ai_service = AIService()